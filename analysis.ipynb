{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_housing.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Baseline Model\n",
    "In order to fit a baseline model for use in comparisons with the more complete model I will conduct these steps:\n",
    "1. Handle Muliticolinearity\n",
    "2. Standardize Variables\n",
    "3. Fit and Validate the Baseline Model\n",
    "\n",
    "## 1. Handle Multicolinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price              False\n",
       "bedrooms           False\n",
       "bathrooms          False\n",
       "sqft_living         True\n",
       "floors             False\n",
       "view               False\n",
       "grade              False\n",
       "sqft_above          True\n",
       "sqft_basement      False\n",
       "lat                False\n",
       "long               False\n",
       "sqft_living15      False\n",
       "WaterFront         False\n",
       "month_sold         False\n",
       "age                 True\n",
       "yrs_reno            True\n",
       "Renovated          False\n",
       "dist_to_Seattle    False\n",
       "Bellevue           False\n",
       "Federal Way        False\n",
       "Kent               False\n",
       "Renton             False\n",
       "Seattle            False\n",
       "rel_size           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_corr = ((abs(data.corr())> .8).sum()>1)\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>age</th>\n",
       "      <th>yrs_reno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sqft_living</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839924</td>\n",
       "      <td>-0.366368</td>\n",
       "      <td>-0.376163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sqft_above</td>\n",
       "      <td>0.839924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.483546</td>\n",
       "      <td>-0.482755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>-0.366368</td>\n",
       "      <td>-0.483546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>yrs_reno</td>\n",
       "      <td>-0.376163</td>\n",
       "      <td>-0.482755</td>\n",
       "      <td>0.938759</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sqft_living  sqft_above       age  yrs_reno\n",
       "sqft_living     1.000000    0.839924 -0.366368 -0.376163\n",
       "sqft_above      0.839924    1.000000 -0.483546 -0.482755\n",
       "age            -0.366368   -0.483546  1.000000  0.938759\n",
       "yrs_reno       -0.376163   -0.482755  0.938759  1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['sqft_living', 'sqft_above', 'age', 'yrs_reno']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid multicolinearity issues I will drop sqft_above, because the information already exists in a combination of sqft_living and sqft_basement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('sqft_above',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardize Variables \n",
    "To standardize the data I'll be using scikit learn's Robust Scalar function because of the existence of significant outliers in the independent variable, price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price'].hist();\n",
    "data.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data into categorical and continuous groups. \n",
    "cat_data = data.select_dtypes(include='uint8')\n",
    "\n",
    "con_data = (data.select_dtypes(exclude='uint8')\n",
    "#Keep certain categorical data separate to use for visuals.\n",
    "            .drop(['Renovated?', 'month_sold', 'nearest_city', \n",
    "                  'waterfront'], axis=1))\n",
    "groups = data[['Renovated?', 'month_sold', 'nearest_city',\n",
    "               'waterfront']]\n",
    "con_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(col):\n",
    "    return (con_data[col]\n",
    "            -con_data[col].mean())/con_data[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the continuous data. \n",
    "scaled_con_data = pd.DataFrame([])\n",
    "for col in con_data.columns:\n",
    "    scaled_con_data[col] = scale(col) \n",
    "scaled_con_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join continuous and categorical data. \n",
    "model_data = scaled_con_data.join(cat_data, how='outer')\n",
    "model_data.head()\n",
    "\n",
    "#Save complete dataset for visualising later.\n",
    "data_fin = model_data.join(groups, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by P-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data.drop('price', axis=1)\n",
    "y = model_data['price']\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,X_test,\n",
    " y_train,y_test)=train_test_split(X,y,test_size=.2,\n",
    "                                  random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "model1 = linreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(model, ind_train, ind_test):\n",
    "    \"\"\"\n",
    "    Print relevant statistics for a model.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Fitted LinearRegression object\n",
    "    ind_train: independent variables for training set\n",
    "    ind_test: independent variables for test set\n",
    "    \"\"\"\n",
    "    pred_y_train = model.predict(ind_train)\n",
    "    pred_y_test = model.predict(ind_test)\n",
    "    \n",
    "    #Print top 5% of variables by size of coefficient.\n",
    "    coefs = []\n",
    "    high_coefs = []\n",
    "    for i in range(0, len(model.coef_)):\n",
    "        coefs.append((model.coef_[i],ind_train.columns[i]))\n",
    "    for coef in coefs:\n",
    "        if abs(coef[0]) > abs(np.quantile(model.coef_,.95)):\n",
    "            high_coefs.append(coef)\n",
    "    print('High Impact Variables:\\n')\n",
    "    for variable in high_coefs:\n",
    "        print('Variable: {}\\nCoefficient: {}\\n'\n",
    "              .format(variable[1],variable[0]))\n",
    "\n",
    "    #Print MSE for the train an test set.\n",
    "    train_mse = mean_squared_error(y_train, pred_y_train)\n",
    "    test_mse = mean_squared_error(y_test, pred_y_test)\n",
    "    print('\\nTrain MSE: {}\\nTest MSE: {}\\nDifference:{}\\n'\n",
    "          .format((train_mse),(test_mse),\n",
    "                  (train_mse-test_mse)))\n",
    "    \n",
    "    #Print R^2 against the test data. \n",
    "    print('Train R^2: {}'\n",
    "          .format((r2_score(y_train,pred_y_train))))\n",
    "    print('Test R^2: {}\\n'\n",
    "          .format((r2_score(y_test,pred_y_test))))\n",
    "    \n",
    "\n",
    "    plotdf = pd.DataFrame([])\n",
    "    plotdf['test_resids'] = pred_y_test-y_test\n",
    "    plotdf['y_test'] = y_test\n",
    "    sns.jointplot(x = 'y_test', y = 'test_resids',\n",
    "                  data=plotdf, kind='kde')\n",
    "    plt.show();\n",
    "    \n",
    "    plt.scatter(pred_y_test, y_test)\n",
    "    plt.plot(pred_y_test, pred_y_test, color='black', \n",
    "             label='Predicted Price')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(model1, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['resids']= pred_y - y\n",
    "X['price']=y\n",
    "X.loc[abs(X['resids'])>2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the MSE and R-squared, the model seems fairly accurate. However, based on the KDE plot of residuals, it is clear that the model could be more generalizable, especially for high value homes. Polynomial relationships may be causing drift. \n",
    "\n",
    "# Train the Model\n",
    "In order to train the model and verify it's validity I will conduct the following steps:\n",
    "\n",
    "1. Find and include interaction features.\n",
    "2. Find and include polynomial features.\n",
    "3. Satisfy Assumptions.\n",
    "4. Validate Model\n",
    "\n",
    "During this process I will continually test the models and eliminate variables that do not serve the model.\n",
    "\n",
    "## Find and Include Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction(col1, col2):\n",
    "    \"\"\"\n",
    "    Plot the regression lines of variables grouped by\n",
    "    high and low values. Non-parellel lines show \n",
    "    interaction of variables.\n",
    "    \n",
    "    Parameters:\n",
    "    col1: pandas Series. Variable to group by.\n",
    "    col2: pandas Series. Variable to plot by\n",
    "    \"\"\"\n",
    "    sample = X_train.join(y_train, how='outer')\n",
    "    \n",
    "    hisample = (sample.loc[sample[col1]\n",
    "                           >sample[col1].quantile(.5)])\n",
    "    losample = (sample.loc[sample[col1]\n",
    "                           <sample[col1].quantile(.5)])\n",
    "    \n",
    "    sns.regplot(x=col2, y='price', data=hisample, \n",
    "                scatter=False, truncate=True, \n",
    "                label='High Values of {}'.format(col1))\n",
    "    sns.regplot(x=col2, y='price', data=losample, \n",
    "                scatter=False, \n",
    "                label='Low Values of {}'.format(col1))\n",
    "    plt.title('Interaction of {} and {}'.format(col1, col2))\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    print('\\n*********************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interactions(n, model, ind_train):\n",
    "    \"\"\"\n",
    "    Returns n most predictive interactions based on low MSE.\n",
    "    \n",
    "    Parameters:\n",
    "    n: int. the number of interactions selected.\n",
    "    model: LinearRegression() object being tested. \n",
    "    ind_train: the independent variables in the training set.\n",
    "    \"\"\"\n",
    "    combos = list(combinations(ind_train.columns, 2))\n",
    "    print('Testing {} combinations.\\n'.format(len(combos)))\n",
    "    inters = [(100,0,0)]*n\n",
    "    temp_X = ind_train.loc[:]\n",
    "    for combo in combos:\n",
    "        temp_X['interaction']=(ind_train.loc[:, combo[0]]\n",
    "                               *ind_train.loc[:, combo[1]])\n",
    "        linreg = LinearRegression()\n",
    "        model = linreg.fit(temp_X, y_train)\n",
    "        y_pred = model.predict(temp_X)\n",
    "        score = round(mean_squared_error(y_train, y_pred),3)\n",
    "        if score < inters[-1][0]:\n",
    "            inters.append((score, combo[0], combo[1]))\n",
    "            inters = sorted(inters, reverse=False)[:n]\n",
    "    for inter in inters:\n",
    "        print('MSE including interaction of {} and {}: {}'\n",
    "              .format(inter[1], inter[2], inter[0]))\n",
    "        plot_interaction(inter[1], inter[2])\n",
    "    fin_inters = [i[1:] for i in inters]\n",
    "    return fin_inters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = find_interactions(20, model1, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that a home's relationship to Seattle is important to this analysis. In order to account for this I will first remove any interactions that showed parellell relationships in the plots or are redundant. In the case of redundancies I will use the interaction of distance rather than city so as to not overly weight any city. \n",
    "\n",
    "I will then create a model that includes six of the most impactful interactions. See complete description of why I reasoned that each interaction warranted inclusion in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Homes south of Seattle: {}'\n",
    "      .format(data[data['lat']<47.608013]['lat'].count()))\n",
    "print('Distance of Farthest Southern home: {}'\n",
    "      .format(47.608013-data['lat'].min()))\n",
    "print('Distance of Farthest Nothern home: {}'\n",
    "      .format(data['lat'].max()-47.608013))\n",
    "data['lat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [('grade', 'Seattle'),('grade', 'lat'),\n",
    "       ('sqft_living', 'Seattle'),('sqft_living', 'lat'),\n",
    "       ('grade', 'Kent'),('long', 'Seattle'),\n",
    "       ('lat', 'Seattle'),('dist_to_Seattle', 'Bellevue'),\n",
    "       ('dist_to_Seattle', 'Kent'),('lat', 'Kent'),\n",
    "       ('sqft_living', 'Kent'),('sqft_living15', 'Kent'),\n",
    "       ('sqft_living15', 'Seattle'),\n",
    "       ('bathrooms', 'dist_to_Seattle')]\n",
    "for inter in bad:\n",
    "    interactions.remove(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interactions(interactions, ind_train, ind_test):\n",
    "    \"\"\"\n",
    "    Use forward selection based on lowest MSE to select\n",
    "    and add most predictive interactions to a new model.\n",
    "    \n",
    "    Parameters:\n",
    "    interactions: list of tuples outputed by find_interaction\n",
    "    function.\n",
    "    ind_train: independent variables training data.\n",
    "    ind_test: independent variables test data.\n",
    "    \n",
    "    Returns:\n",
    "    (new_model, new_x_with_interactions)\n",
    "    \"\"\"\n",
    "    additions = interactions\n",
    "    X_temp_tr = ind_train.loc[:]\n",
    "    X_best_tr = ind_train.loc[:]\n",
    "    X_best_t = ind_test.loc[:]\n",
    "    scores = []\n",
    "    baseline = 100\n",
    "    while additions:\n",
    "        for inter in additions:\n",
    "            X_temp_tr[inter[0]\n",
    "                      +' * '\n",
    "                      +inter[1]]=(X_temp_tr.loc[:, inter[0]]\n",
    "                                  *X_temp_tr.loc[:, inter[1]])\n",
    "            linreg = LinearRegression()\n",
    "            model = linreg.fit(X_temp_tr, y_train)\n",
    "            y_pred = model.predict(X_temp_tr)\n",
    "            score = round(mean_squared_error(y_train,\n",
    "                                             y_pred),5)\n",
    "            scores.append((score, inter[0], inter[1]))\n",
    "            best = sorted(scores)[0]\n",
    "            x_temp_tr = X_best_tr.loc[:]\n",
    "        scores = []\n",
    "        if best[0] <= baseline:\n",
    "            additions.remove(best[1:])\n",
    "            baseline = best[0]\n",
    "            X_best_tr[best[1]\n",
    "                      +' * '\n",
    "                      +best[2]]=(X_temp_tr.loc[:, best[1]]\n",
    "                                 * X_temp_tr\n",
    "                                 .loc[:, best[2]])\n",
    "            X_best_t[best[1]\n",
    "                     +' * '\n",
    "                     +best[2]]=(X_best_t.loc[:, best[1]]\n",
    "                                *X_best_t.loc[:, best[2]])\n",
    "            linreg = LinearRegression()\n",
    "            model = linreg.fit(X_best_tr, y_train)\n",
    "            y_pred = model.predict(X_best_tr)\n",
    "            t_pred = model.predict(X_best_t)\n",
    "            print('Interaction Added: {} * {}'\n",
    "                  .format(best[1], best[2]))\n",
    "            print('Current Train MSE: {}'\n",
    "                  .format(best[0]))\n",
    "            print('Current Test MSE: {}'\n",
    "                  .format(round(mean_squared_error\n",
    "                                (y_test, t_pred),5)))\n",
    "            print('Current Difference: {}\\n'\n",
    "                  .format(round(best[0]\n",
    "                          -round(mean_squared_error\n",
    "                                 (y_test, t_pred),5),5)))\n",
    "        else:\n",
    "            print('complete')\n",
    "            break\n",
    "    linreg = LinearRegression()\n",
    "    new_model = linreg.fit(X_best_tr, y_train)\n",
    "    return(new_model, X_best_tr, X_best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, X_train2, X_test2 = add_interactions(interactions, \n",
    "                                             X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(model2, X_train2, X_test2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and include Polynomial Features\n",
    "To find polynomial features I will iterate through the data, attempting to find data that suggests a polynomial relationship between price and the independent variable. I will then   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = X_train2.join(y_train, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_features():\n",
    "    features = []\n",
    "    for col in X_train2.columns:\n",
    "        scores = []\n",
    "        for degree in range(1,10):\n",
    "            df = pd.DataFrame(X_train2[col])\n",
    "            poly = PolynomialFeatures(degree)\n",
    "            X_poly_train = poly.fit_transform(df)\n",
    "            reg_poly = LinearRegression().fit(X_poly_train,\n",
    "                                              y_train)\n",
    "            y_pred = reg_poly.predict(X_poly_train)\n",
    "            score = round(mean_squared_error(y_train,\n",
    "                                             y_pred),5)\n",
    "            scores.append((score, degree, col))\n",
    "            best_score = sorted(scores)[0] \n",
    "        if best_score[1] > 1:\n",
    "            print('Variable: {} should be factored by {}'\n",
    "                  .format(best_score[2], \n",
    "                          best_score[1]))\n",
    "            features.append(best_score)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_feat = get_polynomial_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train2.reset_index()\n",
    "X_train2.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = X_test2.reset_index()\n",
    "X_test2.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train2.loc[:]\n",
    "X_test3 = X_test2.loc[:]\n",
    "for feat in poly_feat: \n",
    "    X_train3[feat[2]+'^2'] = X_train3[feat[2]]**feat[1]\n",
    "    X_test3[feat[2]+'^2'] = X_test3[feat[2]]**feat[1]\n",
    "    \n",
    "linreg = LinearRegression()\n",
    "model3 = linreg.fit(X_train3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(model3, X_train3, X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(X_train2[feat[2]])\n",
    "    X_poly_train = poly.fit_transform(train)\n",
    "    \n",
    "    test = pd.DataFrame(X_test2[feat[2]])\n",
    "    X_poly_test = poly.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_variables(X,y):\n",
    "    \"\"\"\n",
    "    Forward selects variables by adjusted R^2.\n",
    "    \n",
    "    Parameters:\n",
    "    X: pandas dataframe. independent variables.\n",
    "    y: pandas series. dependent variable.\n",
    "    \n",
    "    Returns:\n",
    "    Optimized model. \n",
    "    \"\"\"\n",
    "current_score, best_score = 0, 0\n",
    "remaining = set(X.columns)\n",
    "selected = pd.DataFrame()\n",
    "while remaining and not current_score > best_score:\n",
    "    scores_with_candidates = []\n",
    "    for candidate in remaining:\n",
    "        test_x = selected.join(X[str(candidate)], how='outer')\n",
    "        score = sm.OLS(y,test_x).fit().rsquared_adj\n",
    "        scores_with_candidates.append((score,candidate))\n",
    "    scores_with_candidates.sort()\n",
    "    best_score, best_candidate = scores_with_candidates.pop()\n",
    "    if current_score < best_score:\n",
    "        remaining.remove(best_candidate)\n",
    "        selected = selected.join(X[str(best_candidate)], how='outer')\n",
    "        current_score = best_score\n",
    "print('Selected Columns: {}'.format(selected.columns))\n",
    "best_vars = sm.add_constant(selected)\n",
    "return best_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate(col):\n",
    "    ind_var = pd.DataFrame(X[col]) \n",
    "    frozen_var = X.drop(col, axis=1)\n",
    "    for fro_col in frozen_var.columns:\n",
    "        frozen_var[fro_col] = frozen_var[fro_col].mean()\n",
    "    iso_X = ind_var.join(frozen_var, how='outer')\n",
    "    return iso_X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
